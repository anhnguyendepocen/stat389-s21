---
title: "Lab 04"
output: html_document
---

```{r, include=FALSE, message=FALSE}
library(tidyverse)
library(ggrepel)
library(smodels)
library(stringi)
library(Matrix)
library(glmnet)
library(cleanNLP)
library(magrittr)

theme_set(theme_minimal())
options(sparse.colnames = TRUE)
options(dplyr.summarise.inform = FALSE)
```

## Median Tract Income

The dataset for today's lab uses U.S. Census data. Our prediction task is to
predict the median income of a census tract (a small area approximately equal
to a neighborhood in a city). There are a lot of variables in this data set,
so it will be good to practice penalized regression with. The code below does
a bit of data cleaning. I did not do this ahead of time because we will need
the full dataset later in the semester. It removes a few variables that I do
not want to use and restricts the data to only the lower-48 states.

```{r, message=FALSE}
set.seed(1)

tract <- read_csv(file.path("data", "tract_median_income.csv")) %>%
  mutate(train_id = if_else(runif(n()) < 0.1, "train", "valid")) %>%
  select(-income_q1, -income_q2, -income_q3, -income_q4, -income_p95) %>%
  filter(!(state %in% c("HI", "AK")))

tract
```

To speed up the model creation time, I have only put 10% of the data in the
training set.

### Example Analysis: CBSA

In each section of this lab, you will create one or more penalized regression
models. In this first section I have written most of the code for you to make
it easier to copy this in the following section (these will be good templates
going forward anyway). To start, let's build a model that only uses the variable
"cbsa_name". This variable provides a name to metropolitan areas in the country.
The `cv.glmnet` function has a few extra parameters that let us see how the
model is running and speeds up the process. We will take about these more next
time.

```{r}
mf <- model.frame(median_income ~ cbsa_name,
                  data = select(tract, -train_id))
mt <- attr(mf, "terms")
y <- model.response(mf)
X <- model.matrix(mt, mf)

X_train <- X[tract$train_id == "train",]
y_train <- y[tract$train_id == "train"]

model <- cv.glmnet(X_train, y_train, alpha = 1, nfolds = 3, trace.it = TRUE)
```

The following code shows all of the non-zero coefficients for the 22nd lambda
value. It will be better to only look at the non-zero terms as our datasets
grow in size.

```{r}
beta <- coef(model, s = model$lambda[22])
beta <- beta[apply(beta != 0, 1, any),,drop=FALSE]
beta
```

Take not of areas seem to have particularly large or small incomes. Does this
generally fit your perception of U.S. cities? **Answer**

In the code below, select only those coefficients for the 10th lambda. Notice
that only a very variables are selected.

```{r, question-01}

```

In the next block, we can see how well this model performs in predicting the
output variable:

```{r}
tract %>%
  mutate(pred = predict(model, newx = X)) %>%
  group_by(train_id) %>%
  summarize(rmse = sqrt(mean((median_income - pred)^2)))
```

We will have more to compare it to in the following sections.

### Model 2: Numeric Variables

For the next model, we we are going to fit a lasso regression using all of the
numeric variables in the dataset as predictors. You can do this by removing the
variables "train_id", "state", "cbsa_name", and "county" and then setting the
formula to be "median_income ~ .".

```{r, question-02}

```

Show all of the coefficients for the 15th value of lambda:

```{r, question-03}

```

Next, look at the 27th lambda:

```{r, question-04}

```

And now show those for the optimal lambda (you can leave off the parameter
`s` to get this).

```{r, question-05}

```

Take a moment to look at the results in the last three questions and understand
what they tell us about the data.

Finally, compute the RMSE of the dataset and compare to the previous model:

```{r, question-06}

```

### Model 3: Numeric Variables with Elastic Net

Now, we will fit the same model as in the previous part, but will add back in
the cbsa_area variable.

```{r, question-07}

```

Look at the coefficients for the 25th value of lambda. Are any of these CBSA
areas? **Answer** No.

```{r, question-08}

```

Moving down, try now to see the 35th value of lambda. You should now see a
few cities come into the model. Are these the same that intially popped up
before?

```{r, question-09}

```

And now compute the RMSE for the model. How does it compare to the previous
two models? **Answer**: It is noticably better than both of the last two.

```{r, question-10}

```

### Model 4: Latitude and Longitude

For our last section, start by creating a regression model that uses just the
lon and lat variables within a lasso regression:

```{r, question-11}

```

Find the RMSE; you will see that it is not as good as the previous models.

```{r, question-12}

```

In the code below, I have created a model that fits a 50x50 grid over the U.S.
and allows for a different coefficent to be applied to each grid point (we
will visualize this in just a moment).

```{r}
mf <- model.frame(median_income ~ cut(lon, breaks = 50) * cut(lat, breaks = 50),
                  data = tract)
mt <- attr(mf, "terms")
y <- model.response(mf)
X <- model.matrix(mt, mf)

X_train <- X[tract$train_id == "train",]
y_train <- y[tract$train_id == "train"]

model <- cv.glmnet(X_train, y_train, alpha = 1, nfolds = 3, trace.it = TRUE)
```

Compute the RMSE of this model. You should see that it does slightly better
than the CBSA model.

```{r, question-13}

```

### Plots!

Using the last model we created (the 50x50 grid of lat and lon), predict the
values of the model and draw a scatter plot with longitude on the x-axis,
latitude on the y-axis, and the points colored by the predictions. I suggest
using a size of 0.2 and the color scale `scale_color_distiller(palette = "Spectral")`.

```{r, question-14}

```

To finish, create the same plot but use the smallest value of lambda to fit
the data to. Notice how the plot looks compared to the previous one.

```{r, question-15}

```
