---
title: "Lab 03"
output: html_document
---

```{r, include=FALSE, message=FALSE}
library(tidyverse)
library(ggrepel)
library(smodels)
library(stringi)
library(Matrix)
library(glmnet)
library(cleanNLP)
library(magrittr)
library(forcats)

theme_set(theme_minimal())
options(dplyr.summarise.inform = FALSE)
```

## Flight Delays

Today's lab will look at a data set of flights. Each flight provides an
indicator of whether the flight arrived at least 15 minutes later than
expected. I have subset the data so that there are an equal number of
delayed flights as their are non-delayed flights.

```{r, message=FALSE}
set.seed(1)

flights <- read_csv(file.path("data", "flights_small.csv")) %>%
  mutate(train_id = if_else(runif(n()) < 0.6, "train", "valid"))
flights
```
### Predicting Delays

To start, using a linear regression model, predict whether a flight will be 
delayed using the variable `dep_hour`, the scheduled (local) hour of departure.
Print out a summary of the model.

```{r, question-01}
model <- flights %>%
  filter(train_id == "train") %>%
  lm(delayed ~ dep_hour, data = .)

summary(model)
```

Does it appear that flights are more likely to be delayed earlier in the day
or later in the day? **Answer** The coefficient is positive, so it seems that
flights are more likely to be delayed later in the day (though keeping in mind
that there will be some oddities for times in the early morning).

Using the model you just built, compute the classification rate on the training
and validation set.

```{r, question-02}
flights %>% 
  mutate(pred = as.numeric(predict(model, newdata = .) > 0.5)) %>%
  group_by(train_id) %>%
  summarize(class_rate = mean(pred == delayed))
```

How well does the model perform relative to random guessing? **Answer** As the
data set is balanced, random guessing would give a classification rate of 0.5.
We have a rate of about 0.57, so better than random but still not particularly
amazing.

Next, build a confusion matrix from the data.

```{r, question-03}
flights %>% 
  mutate(pred = as.numeric(predict(model, newdata = .) > 0.5)) %>%
  select(pred, delayed, train_id) %>%
  table()
```

Now, build a logistic regression model using the same variable. Print out the
model summary again.

```{r, question-04}
model <- flights %>%
  filter(train_id == "train") %>%
  glm(delayed ~ dep_hour, data = ., family = binomial())

summary(model)
```

And compute the classification rate.

```{r, question-05}
flights %>% 
  mutate(pred = as.numeric(predict(model, newdata = ., type = "response") > 0.5)) %>%
  group_by(train_id) %>%
  summarize(class_rate = mean(pred == delayed))
```

How does this compare to the linear regression model? **Answer** In this case,
the models produce the exact same classification rates. Not surprising given
the simplicity of this model.

### Comparing LM and GLM

Next, let's build two models that use departure hour, distance, and carrier to
predict delay. Build one linear model and one logistic model, produce predicted
probabilities for both, and plot them again one another as I did in the notes.

```{r, question-06}
model_lm <- flights %>%
  filter(train_id == "train") %>%
  lm(delayed ~ dep_hour + distance + carrier, data = .)

model_glm <- flights %>%
  filter(train_id == "train") %>%
  glm(delayed ~ dep_hour + distance + carrier, data = ., family = binomial())

flights %>%
  mutate(pred_lm = predict(model_lm, newdata = .)) %>%
  mutate(pred_glm = predict(model_glm, newdata = ., type = "response")) %>%
  ggplot(aes(pred_lm, pred_glm)) +
    geom_point() +
    geom_abline(slope = 1, color = "orange")
```

How would you describe the relationship? **Answer** The two predictions are 
quite similar, with a few small differences for the most extreme predictions.
In those cases the logistic regression predicts a less extreme probability than
the linear model.

There are other options for how to relate the product Xb to the response vector
y. The "canonical" option is the logistic function, but any function that maps
the interval [0, 1] into the real numbers would work. Another possible choice
is an inverse CDF function (if you have not taken probability, do not worry
about the details). If we set the family to `binomial(link = "probit")`, this
uses the inverse of a Gaussian/Normal distribution. Setting the link to by
"cauchit" uses a Cauchy distribution.

Repeat the code you had above, but use a Cauchy link function in place of the
default.

```{r, question-07}
model_lm <- flights %>%
  filter(train_id == "train") %>%
  lm(delayed ~ dep_hour + distance + carrier, data = .)

model_glm <- flights %>%
  filter(train_id == "train") %>%
  glm(delayed ~ dep_hour + distance + carrier,
      data = .,
      family = binomial(link = "cauchit"))

flights %>%
  mutate(pred_lm = predict(model_lm, newdata = .)) %>%
  mutate(pred_glm = predict(model_glm, newdata = ., type = "response")) %>%
  ggplot(aes(pred_lm, pred_glm)) +
    geom_point() +
    geom_abline(slope = 1, color = "orange")
```

How does this compare to the original curve? **Answer** While still similar,
the Cauchy link function differs from the linear regression more extremely and
over the entire range. 

### Precision and Recall

So far we have used classification rate as our main metric for prediction
purposes. This is the one we will use most this semester because it is the
most relevant to multi-class prediction tasks, which will be our focus. However,
there are other metrics which can be very useful when doing binary
classification, particularly when we have a preference for a certain type of
error (falsely classifying 1s as 0s, or vice-versa). Assume that we have two
classes named 0 and 1, we can define the two quantities:

- Precision: For the observations we predict to be 1, what proportion actually are?
- Recall: What proportion of the total number of 1s did we correctly predict?

Notice that these two quantities are in competition with one another. If we 
predict that every flight would be delayed, we have a perfect recall but a
very low precision. Likewise, predicting none of the flights to be delayed
gives a perfect precision but a recall of 0%. We have another metrics that
gives a balance between the two extremes called the F1 score. It is defined as
the harmonic mean of the precision and recall:

  F1 = 2 * (precision * recall) / (precision + recall)

In this section you are going to compute the precision recall, and F1 number 
for a specific model. Let's start by building a logistic regression model 
using the variables: departure hour, arrival house, carrier, origin,
destination, distance, and weekday. Print out the classification rate so that
we have something to compare to.

```{r, question-08}
model <- flights %>%
  filter(train_id == "train") %>%
  glm(delayed ~ arr_hour +
                dep_hour +
                carrier +
                origin +
                dest +
                distance +
                weekday,
      data = ., family = binomial())

flights %>% 
  mutate(pred = as.numeric(predict(model, newdata = ., type = "response") > 0.5)) %>%
  group_by(train_id) %>%
  summarize(class_rate = mean(pred == delayed))
```

Next, using the definitions above, compute the precision, recall, and F1 
scores for the training and testing sets. You'll probably need some of the
methods described in the **Using R to Manipulate and Visualize Data** linked
to from the website.

```{r, question-09}
flights %>%
  mutate(pred = as.numeric(predict(model, newdata = ., type = "response") > 0.5)) %>%
  group_by(train_id) %>%
  summarize(precision = sum(pred == 1 & delayed == 1) / sum(pred == 1),
            recall = sum(pred == 1 & delayed == 1) / sum(delayed == 1)) %>%
  mutate(f1 = 2 * (precision * recall) / (precision + recall))
```

You should see that the model provides a balance of precision and recall; both
should be very similar. Now, copy the code from your previous solution, but 
classify points as a delay if the predicted value is greater than 0.3 (rather
than 0.5):

```{r, question-10}
flights %>%
  mutate(pred = as.numeric(predict(model, newdata = ., type = "response") > 0.45)) %>%
  group_by(train_id) %>%
  summarize(precision = sum(pred == 1 & delayed == 1) / sum(pred == 1),
            recall = sum(pred == 1 & delayed == 1) / sum(delayed == 1)) %>%
  mutate(f1 = 2 * (precision * recall) / (precision + recall))
```

What happens to the precision, recall, and F1 score? **Answer** The precision
drops a bit, the recall increases significantly, and the F1 score increases
by a good amount as well.

Now, repeat with the cut-off 0.7:

```{r, question-11}
flights %>%
  mutate(pred = as.numeric(predict(model, newdata = ., type = "response") > 0.7)) %>%
  group_by(train_id) %>%
  summarize(precision = sum(pred == 1 & delayed == 1) / sum(pred == 1),
            recall = sum(pred == 1 & delayed == 1) / sum(delayed == 1)) %>%
  mutate(f1 = 2 * (precision * recall) / (precision + recall))
```

What happens now? **Answer** The precision increases to 0.7 but the recall drops
to 0.14. The F1 score is much less than before.

Can you think of applications where we would prefer each of these models?

### Your own model

Finally, see if you can improve on the best classification rate by changing 
the variables. Try to add interactions or polynomial terms. You can also wrap
numeric variables in the function `factor()` to convert them to a categorical
variable if you think it will help.

**Note:** I found the best model to include a factor on month and weekday and 
polynomial expansion on the arrival and departure hours.

```{r, question-12}
model <- flights %>%
  filter(train_id == "train") %>%
  glm(delayed ~ poly(arr_hour, 3) +
                poly(dep_hour, 3) +
                carrier +
                origin +
                dest +
                distance +
                factor(month) +
                factor(weekday),
      data = ., family = binomial())

flights %>% 
  mutate(pred = as.numeric(predict(model, newdata = ., type = "response") > 0.50)) %>%
  group_by(train_id) %>%
  summarize(class_rate = mean(pred == delayed))
```
