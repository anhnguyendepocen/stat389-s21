---
title: "Notebook 04: Penalized Regression"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "note-style.css"
---

```{r message=FALSE}
library(tidyverse)
library(forcats)
library(ggrepel)
library(smodels)
library(glmnet)

theme_set(theme_minimal())
options(dplyr.summarise.inform = FALSE)
options(width = 77L)
```

## Thought Experiment

In the article "The relationship between jaw, arm and leg size in
three ethnic groups" (DOI: 10.1002/aja.1001360211), it was determined
that the length of the average human leg is roughly 20% larger than
the length of their arms. We know that the length between someone's
left arm and their right arm are very highly correlated (virtually
the same, depending on your precision). If we collected data and
tried to predict the length of left legs as a function of right arms
and left arms we would have several different choices that seem
to given essentially the same fit.

We could put all of the model weight on the left arm:

$$ \text{left_leg} = \text{left_arm} \cdot 1.2 + \text{right_arm} \cdot 0.0 $$

All on the right arm:

$$ \text{left_leg} = \text{left_arm} \cdot 0.0 + \text{right_arm} \cdot 1.2 $$

Split it between both arms:

$$ \text{left_leg} = \text{left_arm} \cdot 0.6 + \text{right_arm} \cdot 0.6 $$

Or, do something quite a bit more complex:

$$ \text{left_leg} = \text{left_arm} \cdot 101.2 - \text{right_arm} \cdot 100 $$

If our measurements for left arms and right arms were *exactly* the same,
there would be no way of distinguishing this infinite set of choices. This
would then be a *rank deficient* model. If just one person has a different
left and right arm measurement (perhaps an old injury or we are using very
precise measuring tool), there will be a best model in the ordinary least
squares sense. However, this may not match what the best model should be
for future predictions.

What might be the best choice? Either of the models that put all of the weight
on one arm is easier to apply because we need only one variable (not two). At
the same time, the model with an even split will even out any measurement or
data error made in measuring one of the two arms. At least, in general, it does
not seem to be a good idea to use the last model with a negative coefficient.
Before going forward with this idea, let's look at an actual data set.

## City Tempurature Data

Today we will start by looking at a small data set of temperatures
from 327 cities across the world.

```{r, message = FALSE}
temps <- read_csv("data/city_temps_yr.csv")
temps
```

Our goal is to predict the average daily high temperature for the
month of May using the average daily temperatures for the other
eleven months of the year. This is not particularly hard but will
be a great illustration of linear regression regularization.

### Ordinary regression

We can start by predicting the average daily high in May as a
function of all of the other eleven months. We can understand these coefficients
best by plotting them over the course of the year (I have hidden to code because
it is only useful in this one example):

```{r, echo=FALSE}
model <- lm(may ~ jan + feb + mar + apr + jun + jul + aug + sep +
                  oct + nov + dec, data = temps)

tibble(
  month = names(coef(model)[-1]),
  val = coef(model)[-1],
  gr = c(rep(0, 4L), rep(1, 7L))
  ) %>%
  ggplot(aes(factor(month, levels = tolower(month.abb)), val)) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    geom_path(aes(group = gr), size = 1) +
    geom_point(size = 3) +
    theme_minimal() +
    scale_x_discrete(limits = tolower(month.abb)) +
    xlab("Month") + ylab("Coefficent")
```

As you might expect, April and Jun have the largest coefficients.
You may not have thought that other months would have negative
weights. What is going on here? These variables have fairly high
correlations, so we can get a fairly predictive model by combining
positive weights on some variables and negative weights on others.
This is a less-extreme version of the thought experiment we openned
with.

## Regularization

The tendency of linear regression to overfit the data in the presence of
highly-correlated variables or sets of variables is a major issue in
predictive modeling. So far our only option has been to control this
by limiting the number of variables in a model, either directly or by
limiting the number of interactions or basis expansion terms.

Regularization is a more direct approach to limiting the complexity of
a model. In general, it works by optimizing the fit of the data (such as
the sum of squared errors) plus the complexity of the model. A tuning parameter
gives the balance between the two measurements:

$$ \text{FIT} + \lambda \times \text{COMPLEXITY} $$

Different choices of how to measure the complexity of the linear regression
yield different types of models, which we will now investigate.

### Ridge regression

Ridge regression sets the complexity to be the sum of the squared regression
parameters. Specifically, we have the following equation to minimize:

$$ \sum_i \left( y_i - x_i^t b \right)^2 + \lambda \cdot \sum_j b_j^2 $$

If lambda is very large, the best model will have no coefficients.
As lambda approaches zero, the ridge vector will limit towards
the ordinary least squares solution.

We will see how to write the code to fit this model in a moment. Let's just
look at the output for now:

```{r, echo=FALSE}
mf <- model.frame(may ~ jan + feb + mar + apr + jun + jul + aug + sep +
                  oct + nov + dec -1, data = temps)
mt <- attr(mf, "terms")
y <- model.response(mf)
X <- model.matrix(mt, mf)

model <- cv.glmnet(X, y, alpha = 0)

tibble(
  month = rownames(coef(model))[-1],
  val = coef(model)[-1],
  gr = c(rep(0, 4L), rep(1, 7L))
  ) %>%
  ggplot(aes(factor(month, levels = tolower(month.abb)), val)) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    geom_path(aes(group = gr), size = 1) +
    geom_point(size = 3) +
    theme_minimal() +
    scale_x_discrete(limits = tolower(month.abb)) +
    xlab("Month") + ylab("Coefficent")
```

Now, the weights are all positive and spread out over the
course of the year. There is still, however, a peak near
April and June. What has happened is two fold: the penalty
as made it "not worth" making offsetting negative weights
and positive weights even if this slightly decreases the
RMSE. The penalty is simply too great. Secondly, the
ridge penalty in particular prefers many small weights
compared to a few larger weights. That is why the values
are spread throughout the year.

### Lasso Regression

Lasso regression uses the sum of absolute values of the regression coefficents
as a penalty:

$$ \sum_i \left( y_i - x_i^t b \right)^2 + \lambda \cdot \sum_j | b_j | $$

The behavior of the lasso regression has a very special property
that can be extremely useful in predictive modeling. It tends to put a weight
of exactly zero on terms that do not seem particularly important to the
model predictions:

```{r, echo=FALSE}
mf <- model.frame(may ~ jan + feb + mar + apr + jun + jul + aug + sep +
                  oct + nov + dec -1, data = temps)
mt <- attr(mf, "terms")
y <- model.response(mf)
X <- model.matrix(mt, mf)

library(glmnet)
model <- cv.glmnet(X, y, alpha = 1)

tibble(
  month = rownames(coef(model))[-1],
  val = coef(model)[-1],
  gr = c(rep(0, 4L), rep(1, 7L))
  ) %>%
  ggplot(aes(factor(month, levels = tolower(month.abb)), val)) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    geom_path(aes(group = gr), size = 1) +
    geom_point(size = 3) +
    theme_minimal() +
    scale_x_discrete(limits = tolower(month.abb)) +
    xlab("Month") + ylab("Coefficent")
```

The model has made every weight other than April and June set
to exactly zero. As with ridge regression, the penalty is too
great to make canceling negative weights and postive weights.
Unlike ridge regression, the lasso penalty does not prefer
many small weights to fewer large weights. Therefore, the model
will pick only those variables which are most strongly correlated
with the response; here, these are April and June.

The magic of lasso regression is that it sets many terms exactly
to zero. This is accomplished because the absolute value does not
have a derivative at zero. Therefore, the target function has
many critical points where beta coefficents are equal to zero
and a non-zero chance of setting any beta value to zero.

Which model would the lasso regression prefer in our left leg
prediction example? It would put all of the weight
on one arm, whichever one had a slightly higher correlation with
the response.

### Elastic Net

The elastic net refers to a collection of models that sit in-between ridge and
lasso regression. It is defined as a weighted sum of the absolute value of
the coefficients and the square of the coefficients.

$$ \sum_i \left( y_i - x_i^t b \right)^2 + \lambda \cdot \sum_j | b_j | \cdot (\alpha) + | b_j |^2 \cdot (1 - \alpha) $$

Here is the fit for our data with alpha equal to 0.2:

```{r, echo=FALSE}
mf <- model.frame(may ~ jan + feb + mar + apr + jun + jul + aug + sep +
                  oct + nov + dec -1, data = temps)
mt <- attr(mf, "terms")
y <- model.response(mf)
X <- model.matrix(mt, mf)

library(glmnet)
model <- cv.glmnet(X, y, alpha = 0.2)

tibble(
  month = rownames(coef(model))[-1],
  val = coef(model)[-1],
  gr = c(rep(0, 4L), rep(1, 7L))
  ) %>%
  ggplot(aes(factor(month, levels = tolower(month.abb)), val)) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    geom_path(aes(group = gr), size = 1) +
    geom_point(size = 3) +
    theme_minimal() +
    scale_x_discrete(limits = tolower(month.abb)) +
    xlab("Month") + ylab("Coefficent")
```

With a value of 0.2, the model puts some weight on March, April, June, and July,
a true blend between the ridge and lasso models

## The glmnet package

The **glmnet** package provides a function for fitting the elastic net model, as
well as the specific cases of the lasso and ridge regression. It requires that
we provide a matrix form of our data. To actually fit the data, we will use the
`cv.glmnet` function. To see how this works, let's look at the California house
price data set again, starting by constructing the training data:

```{r, message = FALSE}
set.seed(1)
ca <- read_csv("data/ca_house_price.csv") %>%
  mutate(train_id = if_else(runif(n()) < 0.6, "train", "valid"))

mf <- model.frame(median_house_value ~ . -1,     # the "." in the model
                  data = select(ca, -train_id))  # means "use everything!"
mt <- attr(mf, "terms")
y <- model.response(mf)
X <- model.matrix(mt, mf)

X_train <- X[ca$train_id == "train",]
y_train <- y[ca$train_id == "train"]
```

Next will run an elastic net regression on the data with alpha equal to 0.9:

```{r}
model <- cv.glmnet(X_train, y_train, alpha = 0.9)
```

The function intelligently picks a set of 100 values of lambda
to fit the elastic net to. We can see all of these by looking
at the lambda parameter of the model

```{r}
model$lambda
```

The `predict` function, by default, chooses the "optimal" value of lambda (we
will see in the next set of notes how this works and what it means). We can
use this code to evaluate the RMSE of the estimator:

```{r}
ca %>%
  mutate(pred = predict(model, newx = X)) %>%
  group_by(train_id) %>%
  summarize(rmse = sqrt(mean((median_house_value - pred)^2)))
```

The RMSE here is slightly better than the best unpenalized model I was able to
find in my solutions to Lab 02.

The `coef` function also, be default provides the coefficents for the best value
of lambda. We can manually pass the option `s` to specify which value of lambda
we want the parameters for (that it is called `s` and not `lambda` has always
been a pet-peeve of mine). We can pick any value between the largest and
smallest lambda parameters, though it generally makes sense to pick values
where the model was actually fit.

The primary reason for looking at other values of lambda is to
see which variables are included when the penalty is very
high. Here, at the 10th value of lambda (remember, they are
in descending order), we see that only `mean_household_income`
is included (note: a dot is a zero):

```{r}
coef(model, s = model$lambda[10])
```

Setting it to the 14th value, shows several additional variables
that seem particularly important:

```{r}
coef(model, s = model$lambda[14])
```

Notice, however, that latitude and longitude are not popping
up because we would need several interaction orders for them
to become the most predictive variables.
