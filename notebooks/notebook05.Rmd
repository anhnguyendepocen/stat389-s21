---
title: "Notebook 05: Cross-Validation and Multinomial Regression"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "note-style.css"
---

```{r message=FALSE}
library(tidyverse)
library(forcats)
library(ggrepel)
library(smodels)
library(glmnet)

theme_set(theme_minimal())
options(dplyr.summarise.inform = FALSE)
options(width = 77L)
```

## Binomial and Multinomial elastic net

The "g" in `glmnet` stands for the same type of generalized as in `glm`. We can
fit classification models by adjusting the family function. Let's use the NBA
shot data set as an example. To start, read in the data set and create a model
matrix with all of the variables other than the player name.

```{r, message = FALSE}
set.seed(1)
nba <- read_csv("data/nba_shots.csv") %>%
  mutate(train_id = if_else(runif(n()) < 0.6, "train", "valid"))

mf <- model.frame(fgm ~ . -1, data = select(nba, -player_name, -train_id))
mt <- attr(mf, "terms")
y <- model.response(mf)
X <- model.matrix(mt, mf)

X_train <- X[nba$train_id == "train",]
y_train <- y[nba$train_id == "train"]
```

We can fit a binomial model with glmnet, but it will be easier to generalize
to other data sets to use a multinomial regression model. It will work in the
case where the variable we are trying to predict can take on more than two
categories.

Multinomial regression, as implemented by glmnet, works by fitting a separate
logistic regression model, where we have a set of coefficients for each
category. Prediction works be applying each of the models and seeing which
has the greatest probability.

Here is the syntax to run this model using the elastic net:

```{r}
model <- cv.glmnet(X_train, y_train, alpha = 0.9, family = "multinomial")
```

Now, let's take a look at the non-zero coefficients in the model for a specific
value of lambda. The code here is something that we will use many times,
changing only the parameter `s` in the first line.

```{r}
temp <- coef(model, s = model$lambda[22])
beta <- Reduce(cbind, temp)
beta <- beta[apply(beta != 0, 1, any),]
colnames(beta) <- names(temp)
beta
```

The first column indicates those features that classify the 0 category (shot
missed) and the second features that classify the 1 category (shot made). In
this case they are the same magnitude and opposite signs. The different columns
will include different information in the case of 3 or more categories, as you
will see in the lab for today.

And then, to get the classification rate, we use the predict function and set
the type argument equal to "class":

```{r}
nba %>%
  mutate(pred = predict(model, newx = X, type = "class")) %>%
  group_by(train_id) %>%
  summarize(class_rate = mean(pred == fgm))
```

### Scaling and intercepts (some technical details)

As we wrote the lasso, ridge regression, and elastic net the
scale of the predictor variables would have a large impact on
the model. We did not focus on this because **glmnet** always
scales the columns to have unit variance and zero mean. Generally,
we do not have to worry about this and I have rarely found a
reason to modify this default behavior.

The elastic net function
also puts in a manual intercept for us. The intercept is
treated differently because it does not have a penalty. Again,
this is almost always the preferred behavior and you will likely
have no reason to change it. If you accidentally do put in an
intercept into the model, it will be silently ignored (why would
the model put a weight on your intercept, which is penalized,
rather than the internal one which is not?).

### Cross-validation

How does the elastic net determine which value of lambda
is the best? It uses a process called cross-validation
(that's where the "cv" comes from) where the training set
itself is used to do automated validation of the model.

Cross validation works as follows. Here I am using 10-fold
validation, but you can modify to have k-fold validation:

- start with an initial choice of lambda
- assign every training observation randomly to one of
ten buckets
- fit a model using only data from buckets 2-10 with
the first lambda value. Use this to predict the values
from bucket 1.
- fit a model using only data from buckets 1, and 3-10.
Use this to predict the values from bucket 2.
- repeat eight more time to get predictions for the other
buckets
- you now have a prediction for each point in the training
data. Compute the RMSE or other target metric on this set.
- repeat for all values of lambda (100, by default)

The "best" lambda is the one that is found to minimize the
target metric from cross-validation. A final model is built
using all of the data and that is the one that we get as
an output.

Let's visualize this by including the player name variable in our model matrix
in order to create a model matrix with a large number of variables.

```{r, message = FALSE}
mf <- model.frame(fgm ~ . -1, data = select(nba, -train_id))
mt <- attr(mf, "terms")
y <- model.response(mf)
X <- model.matrix(mt, mf)

X_train <- X[nba$train_id == "train",]
y_train <- y[nba$train_id == "train"]

dim(X)
```

And fit the cross-validated elastic net model:

```{r}
model <- cv.glmnet(X_train, y_train, alpha = 0.9, family = "multinomial")
```

Plotting the model visualizes the cross-validation
error for each value of lambda (the dot is the median error and the bars are
confidence intervals):

```{r}
plot(model)
```

We can see the "best" lambda, as well as the other lambda defined by the dashed
line above with this code:

```{r}
c(lambda.min = log(model$lambda.min), lambda.1se = log(model$lambda.1se))
```

The second lambda gives a model that is within one standard
error of the predictions from the "best" model but often has
significantly fewer variables in it. We can see that here:

```{r}
temp <- coef(model, s = model$lambda.min)
beta <- Reduce(cbind, temp)
beta <- beta[apply(beta != 0, 1, any),]
colnames(beta) <- names(temp)
beta
```

```{r}
temp <- coef(model, s = model$lambda.1se)
beta <- Reduce(cbind, temp)
beta <- beta[apply(beta != 0, 1, any),]
colnames(beta) <- names(temp)
beta
```

Often, the best model on new data sets will be the model set with the
lambda with a cross-validation loss 1 standard error greater than the minimum
value. It is also much easier to interpret the results.
